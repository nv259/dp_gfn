# -*- coding: utf-8 -*-
"""GFlowNet_tutorial_JAX.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11NMRXGYsEKmfGgykJ0V0TUET-wazyJ7o
"""

import matplotlib.pyplot as pp
import numpy as np
base_face = lambda: (pp.gca().add_patch(pp.Circle((0.5,0.5),0.5,fc=(.9,.9,0))),
                     pp.gca().add_patch(pp.Circle((0.25,0.6),0.1,fc=(0,0,0))),
                     pp.gca().add_patch(pp.Circle((0.75,0.6),0.1,fc=(0,0,0))))
patches = {
  'smile': lambda: pp.gca().add_patch(pp.Polygon(np.stack([np.linspace(0.2,0.8), 0.3-np.sin(np.linspace(0,3.14))*0.15]).T, closed=False, fill=False, lw=3)),
  'frown': lambda: pp.gca().add_patch(pp.Polygon(np.stack([np.linspace(0.2,0.8), 0.15+np.sin(np.linspace(0,3.14))*0.15]).T, closed=False, fill=False, lw=3)),
  'left_eb_down': lambda: pp.gca().add_line(pp.Line2D([0.15, 0.35], [0.75,0.7], color=(0,0,0))),
  'right_eb_down': lambda: pp.gca().add_line(pp.Line2D([0.65, 0.85], [0.7,0.75], color=(0,0,0))),
  'left_eb_up': lambda: pp.gca().add_line(pp.Line2D([0.15, 0.35], [0.7,0.75], color=(0,0,0))),
  'right_eb_up': lambda: pp.gca().add_line(pp.Line2D([0.65, 0.85], [0.75,0.7], color=(0,0,0))),
}
sorted_keys = sorted(patches.keys())

def draw_face(face):
  base_face()
  for i in face:
    patches[i]()
  pp.axis('scaled')
  pp.axis('off')

f, ax = pp.subplots(1,2)
pp.sca(ax[0])
draw_face(['smile', 'left_eb_down', 'right_eb_down'])
pp.sca(ax[1])
draw_face(['frown', 'left_eb_up', 'right_eb_up'])

for idx, key in enumerate(sorted_keys):
    print(idx, key)

def has_overlap_v2(face):
    if 1 in face and 2 in face:
        return True
    if 3 in face and 4 in face:
        return True
    if 0 in face and 5 in face:
        return True

    return False

def face_reward_v2(face):
    if has_overlap_v2(face):
        return 0

    eyebrows = 1, 2, 3, 4
    if sum([i in face for i in eyebrows]) != 2:
        return 0

    if 0 in face:
        return 1
    if 5 in face:
        return 2

    return 0

def has_overlap(face):
  # Can't have two overlapping eyebrows!
  if 'left_eb_down' in face and 'left_eb_up' in face:
    return True
  if 'right_eb_down' in face and 'right_eb_up' in face:
    return True
  # Can't have two overlapping mouths!
  if 'smile' in face and 'frown' in face:
    return True
  return False

def face_reward(face):
  if has_overlap(face):
    return 0
  eyebrows = 'left_eb_down', 'left_eb_up', 'right_eb_down', 'right_eb_up'
  # Must have exactly two eyebrows
  if sum([i in face for i in eyebrows]) != 2:
    return 0
  # We want twice as many happy faces as sad faces so here we give a reward of 2 for smiles
  if 'smile' in face:
    return 2
  if 'frown' in face:
    return 1  # and a reward of 1 for frowns
  # If we reach this point, there's no mouth
  return 0

#@title
enumerated_states = []
transitions = []
def recursively_enumerate(s):
  if has_overlap(s):
    return
  for i in sorted_keys:
    if i not in s:
      recursively_enumerate(s+[i])
  enumerated_states.append(s)
  transitions.append((s[:-1], s))
recursively_enumerate([])
unique = []
for i in map(set, enumerated_states):
  if i not in unique:
    unique.append(i)
enumerated_states = sorted(map(tuple, unique))

lens = [len([i for i in enumerated_states if len(i) == j]) for j in range(4)]
levels = [sorted([i for i in enumerated_states if len(i) == j]) for j in range(4)]
f = pp.figure(figsize=(8, 8))
def face_hash(face):
  return tuple([i in face for i in sorted_keys])
face2pos = {}
for i, (level, L) in enumerate(zip(levels, lens)):
  for j, face in enumerate(level):
    ax = f.add_axes([j/L, i/4, 1/L, 1/6])
    draw_face(face)
    face2pos[face_hash(face)] = (j/L+0.5/L, i/4)
ax = f.add_axes([0,0,1,1])
pp.sca(ax)
pp.gca().set_facecolor((0,0,0,0))
pp.xlim(0,1)
pp.ylim(0,1)
for a, b in transitions[1:]:
  pa, pb = face2pos[face_hash(a)], face2pos[face_hash(b)]
  if not len(b): continue
  lb = int(pb[1] * 4)
  la = int(pa[1] * 4)
  ws = [1/6,1/6,0.13,0.11]
  pp.arrow(pa[0],pa[1]+ws[la],pb[0]-pa[0],pb[1]-pa[1]-ws[lb], head_width=0.01, width=0.003, ec=(0.,0.5,0),fc=(0,0,0),
           length_includes_head=True)
  pp.axis('off')

# !pip install dm-haiku

import jax
import haiku as hk
import jax.numpy as jnp
import tqdm
import optax

def face_to_tensor(faces):
    x = jnp.arange(0, 6)
    return jnp.array([[i in face for i in x] for face in faces]).astype(float)

class DenseBlock(hk.Module):
    def __init__(self, output_dim, hiddens=None, act_fn=jax.nn.leaky_relu, name='dense'):
        super().__init__(name)
        self.output_dim = output_dim
        self.act_fn = act_fn
        self.hiddens = hiddens

    def __call__(self, x):
        w_init = hk.initializers.RandomNormal()

        if not self.hiddens:
            self.hiddens = [(x.shape[-1] + self.output_dim) // 2]

        for hidden in self.hiddens:
            x = hk.Linear(hidden, w_init=w_init)(x)
            x = self.act_fn(x)

        x = hk.Linear(self.output_dim, w_init=w_init)(x)

        return x

from jax import random


MASKED_VALUE = -1e5

def mask_logits(logits, mask):
    return mask * logits + (1 - mask) * MASKED_VALUE


def random_choice(key, probas, mask, delta):
    # Sample from the distribution
    uniform = random.uniform(key, shape=(delta.shape))
    cum_probas = jnp.cumsum(probas, axis=-1)
    samples = jnp.sum(cum_probas < uniform, axis=-1)

    # mask = mask.reshape(mask.shape[0], -1)
    # is_valid = jnp.take_along_axis(mask, samples, axis=1)    # TODO: Implement precautionary measure for potential failure in sampling actions

    return samples


def sample_action(key, log_pi, mask, delta, ret_backward=False):
    key, subkey1, subkey2 = random.split(key, 3)

    log_uniform = uniform_log_policy(mask)
    is_exploration = random.bernoulli(subkey1, p=delta, shape=(delta.shape))

    log_pi = jnp.where(is_exploration, log_uniform, log_pi)

    actions = random_choice(subkey2, jnp.exp(log_pi), mask, delta)

    log_pF = log_pi[actions]

    if ret_backward:
        log_pB = uniform_log_policy(mask, is_forward=False)
        return key, actions, log_pF, log_pB

    return key, actions, log_pF


def uniform_log_policy(mask, is_forward=True):
    num_valid_actions = jnp.sum(mask, axis=-1, keepdims=True)
    log_pi = -jnp.log(num_valid_actions)

    if is_forward:
        log_pi = mask_logits(log_pi, mask)
    else:
        log_pi = log_pi.squeeze(-1)

    return log_pi

class TBModel(hk.Module):
    def __init__(self, hiddens=[512], act_fn=jax.nn.leaky_relu):
        super().__init__(name='gfn')
        self.hiddens = hiddens

    def __call__(self, key, x, mask, delta):
        forward_logits = DenseBlock(6, self.hiddens, name='forward')(x)
        backward_logits = DenseBlock(6, self.hiddens, name='backward')(x)

        log_pFs = self.log_policy(forward_logits, mask)
        key, action, log_pF = sample_action(key, log_pFs, mask, delta)
        log_pB = self.log_policy(backward_logits, 1 - mask)
        # self.logZ = hk.get_parameter('logZ', [], init=jnp.ones)

        return key, action, log_pF, log_pB

    def log_policy(self, logits, mask):
        """Generate the log version of Policy from the input logits and filtering mask.

        Args:
            logits (jnp.DeviceArray): Input logits
            mask (np.ndarray): Mask of the valid actions

        Returns:
            jnp.DeviceArray: Log Policy with shape similar to logits
        """
        mask = mask.reshape(logits.shape)
        logits = mask_logits(logits, mask)
        log_pi = jax.nn.log_softmax(logits, axis=-1)

        return log_pi

def edge_flow_prediction_fn(key, x, mask, delta, hiddens=None, act_fn=jax.nn.leaky_relu):
    key, action, log_pF, log_pB = TBModel(hiddens, act_fn)(key, x, mask, delta)

    return key, action, log_pF, log_pB

def logZ_fn(x):
    log_Z = hk.get_parameter(name='logZ', shape=(1,), init=jnp.ones)
    return log_Z


key = jax.random.PRNGKey(0)
face = []
state = jnp.zeros((6, ))
delta = jnp.array(0.00)

gfn = hk.without_apply_rng(hk.transform(edge_flow_prediction_fn))
gfn_params = gfn.init(key, key, state, 1 - state, delta)
gfn = jax.vmap(gfn.apply, in_axes=(None, 0, 0, 0, 0, ))

logZ = hk.without_apply_rng(hk.transform(logZ_fn))
Z_params = logZ.init(key, state)
logZ = jax.vmap(logZ.apply, in_axes=(None, 0))

optimizer = optax.multi_transform(
    {
        'gfn': optax.adam(3e-3),
        'logZ': optax.adam(3e-3)
    },
    ('gfn', 'logZ')
)
opt_state = optimizer.init((gfn_params, Z_params))

@jax.jit
def trajectory_balance_loss(traj_log_pF, traj_log_pB, log_R, log_Z):
    log_Z = log_Z.squeeze(-1)
    return jnp.power((log_Z + traj_log_pF - log_R - traj_log_pB), 2).mean()

def loss(key, gfn_params, Z_params, delta):
    log_Z = jax.jit(logZ)(Z_params, jnp.zeros((16, 6)))
    
    if (log_Z > 2.4845).any():
        print('debug')     
    
    key, complete_states, traj_log_pF, traj_log_pB = sample(key, gfn_params, delta)
    log_R = jnp.log(jnp.array([face_reward_v2(face) for face in complete_states])).clip(-20)
    error = trajectory_balance_loss(traj_log_pF, traj_log_pB, log_R, log_Z)

    # log_R = jnp.log(jnp.array(face_reward_v2(face) for face in complete_states))

    return error, (key, log_R, log_Z, error)

def sample(key, params, delta):
    keys = jax.random.split(key, 16)
    faces = jnp.array([[] for _ in range(16)])
    states = jnp.zeros((16, 6))
    deltas = jnp.array([delta] * 16)

    traj_log_pFs = jnp.zeros((16, ))
    traj_log_pBs = jnp.zeros((16, ))

    for step in range(3):
        keys, actions, log_pFs, log_pBs = jax.jit(gfn)(params, keys, states, 1 - states, deltas)
        traj_log_pFs += log_pFs

        if step > 0:
            log_pB = jnp.take_along_axis(
                log_pBs, (prev_actions)[..., jnp.newaxis], axis=1
            ).squeeze(-1)
            traj_log_pBs += log_pB

        faces = jnp.concatenate([faces, actions[..., jnp.newaxis]], axis=1)
        states = face_to_tensor(faces)
        prev_actions = actions.copy()

    keys, actions, log_pFs, log_pBs = jax.jit(gfn)(params, keys, states, 1 - states, deltas)
    log_pB = jnp.take_along_axis(
        log_pBs, (prev_actions)[..., jnp.newaxis], axis=1
    ).squeeze(-1)
    traj_log_pBs += log_pB
    
    return keys[0], faces, traj_log_pFs, traj_log_pBs

def train(gfn_params, Z_params, opt_state):
    losses = []
    sampled_faces = []
    logZs = []
    exploration_schedule = jax.jit(
        optax.linear_schedule(
            init_value=0.000,
            end_value=0.0000,
            transition_steps=500,
            transition_begin=2000,
        )
    )

    key = jax.random.PRNGKey(0)
    for episode in tqdm.tqdm(range(50000), ncols=40):
        delta = exploration_schedule(episode)
        grads, logs = jax.grad(loss, argnums=(1, 2), has_aux=True)(key, gfn_params, Z_params, delta)

        update, opt_state = optimizer.update(grads, opt_state, (gfn_params, Z_params))
        gfn_params, Z_params = optax.apply_updates((gfn_params, Z_params), update)
        key = logs[0]
        logZs.append(logs[2].mean().item())
        print(' ', logs[1].mean(), logs[2].mean(), logs[3])
        losses.append(logs[3].item())

    return losses

train(gfn_params, Z_params, opt_state)

