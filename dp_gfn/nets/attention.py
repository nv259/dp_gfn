import torch.nn as nn
import torch.nn.functional as F
import numpy as np


class LinearMultiHeadAttention(nn.Module):
    def __init__(self, d_model, n_heads):
        super(LinearMultiHeadAttention, self).__init__()
    
    def forward(self, x):
        pass

    
