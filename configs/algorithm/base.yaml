train:
    n_grad_accumulation_steps: 4    # not yet implemented
    max_steps: 100000
    eval_on_train: False
    exploration_rate: 0.05
    stimulated_annealing: False     # not yet implemented
    clip_grad: 10   # not yet implemented

    optimizer:
        name: Adam
        gflownet_lr: 1e-3
        Z_lr: 3e-3

        bert_factor: 0.0  # pretrained lr 

    lr_scheduler: 
        _target_: null  # warmup or so 
        start_lr: 1e-3
        end_lr: 0
        decay_step: null
        power: null

    exploration_scheduler: 
        init_value: 0.005
        end_value: 0.
        transition_steps: 10000
        transition_begin: 5000

    eval_every_n: 1000
    save_every_n: 5000

    backward_policy: null   # fix uniform by default
    score_fn: null          # graph edit distance by default
eval:
    batch_size: 16

reward_scale_factor: 10