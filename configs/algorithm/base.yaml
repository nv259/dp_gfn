train:
    exp_temp: 1.0
    rand_coef: 0.0
    p_init: 1.0
    n_grad_accumulation_steps: 4    # not yet implemented
    syn_batch_size: 0
    max_steps: 100000
    eval_on_train: False
    exploration_rate: 0.05
    clip_grad: 10   # not yet implemented

    optimizer:
        name: Adam
        gfn_lr: 1e-4
        Z_lr: 5e-4

        bert_factor: 0.0  # pretrained lr 

    lr_scheduler: 
        start_lr: 1e-3
        end_lr: 0
        decay_step: null
        power: null

    exploration_scheduler: 
        init_value: 0.005
        end_value: 0.
        transition_steps: 10000
        transition_begin: 5000

    eval_every_n: 1000
    save_every_n: 5000

    backward_policy: null   # fix uniform by default
    score_fn: null          # graph edit distance by default

reward_scale_factor: 10